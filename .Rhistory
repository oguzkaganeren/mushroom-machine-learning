source(file = "Preparation.R") # just shows worked dataset after data preparation.
View(mushroom)
set.seed(579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*0.8) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
library(arm)
try1 = bayesglm(formula = class ~ ., data = train, family=binomial(link='logit'))
#include data preparation file
source(file = "Preparation.R") # just shows worked dataset after data preparation.
set.seed(579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*0.8) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
library(arm)
try1 = bayesglm(formula = class ~ ., data = train, family=binomial(link='logit'))
glm.probs <- predict(try1, type = "response")
glm.probs[1:5]
glm.pred  <- ifelse(glm.probs > 0.5,"p","e")
misClasificError <- mean(glm.pred != train$class)
print(paste('Accuracy',1-misClasificError))
#include data preparation file
source(file = "Preparation.R") # just shows worked dataset after data preparation.
set.seed(579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*0.8) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
library(arm)
try1 = bayesglm(formula = class ~ ., data = train, family=binomial(link='logit'))
#include data preparation file
source(file = "Preparation.R") # just shows worked dataset after data preparation.
#show current dataset
head(mushroom)
summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*0.8) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
library(arm)
try1 = bayesglm(formula = class ~ ., data = train, family=binomial(link='logit'))
glm.probs <- predict(try1, type = "response")
glm.probs[1:5]
glm.pred  <- ifelse(glm.probs > 0.5,"p","e")
misClasificError <- mean(glm.pred != train$class)
print(paste('Accuracy',1-misClasificError))
fitted.results <- predict(try1,newdata=subset(test),type='response')
fitted.results <- ifelse(fitted.results > 0.5,"p","e")
misClasificError <- mean(fitted.results != test$class)
print(paste('Accuracy',1-misClasificError))
#if we calculate the linear regression model, remove family part
pred = bayesglm(formula = class ~ ., data = train, family=binomial(link='logit'))
probs <- exp(pred)/(1+exp(pred)) #gives you probability that y=1 for each observation
probs <- exp(glm.probs)/(1+exp(glm.probs)) #gives you probability that y=1 for each observation
probs
glm.probs[1:5]
mushroom <- read.csv("agaricus-lepiota.csv",header = TRUE) # load dataset
#replace NA values to columns mode
#Mode <- function (x, na.rm) {
#  xtab <- table(x)
#  xmode <- names(which(xtab == max(xtab)))
#  if (length(xmode) > 1) xmode <- ">1 mode"
#  return(xmode)
#}
#for (var in 1:ncol(mushroom)) {
#  mushroom[is.na(mushroom[,var]),var] <- Mode(mushroom[,var], na.rm = TRUE)
#}
#after data preparation of missing values.
#missmap(mushroom, main = "After data preparation of missing values")
ComputeProportion <- function(target,attribute_dataset,Columns,centroid){
len_attr <- length(Columns)
RMSE <- NULL
for(i in 1:len_attr){
tab <- table(target,attribute_dataset[,Columns[i]])
prop <- tab[2,]/(tab[1,]+tab[2,])
centr_prop <- prop - centroid
Err <- sqrt(mean(centr_prop*centr_prop))
RMSE <-rbind(RMSE,Err)
}
norm_err <- (RMSE-min(RMSE))/(max(RMSE)-min(RMSE))
err_mat <- cbind(colnames(mushroom[,Columns]),RMSE)
err_mat <- cbind(err_mat,norm_err)
colnames(err_mat) <- c("Column name","RMSE","Norm. Error")
err_mat
}
proportions_tab <- ComputeProportion(mushroom[,1],mushroom,c(2:23),0.482)
print(proportions_tab)
#stalk_shape ve veil_type en düşük değere sahip
drops <- c("veil_type","stalk_shape") #there is one unique values of veil_type, stalk_shape we can remove this column in our dataset.
mushroom <- mushroom[ , !(names(mushroom) %in% drops)] #remove veil_type,stalk_shape
# remove unncessary data
rm(columnNames,drops)
mushroom <- read.csv("agaricus-lepiota.csv",header = TRUE) # load dataset
#replace NA values to columns mode
#Mode <- function (x, na.rm) {
#  xtab <- table(x)
#  xmode <- names(which(xtab == max(xtab)))
#  if (length(xmode) > 1) xmode <- ">1 mode"
#  return(xmode)
#}
#for (var in 1:ncol(mushroom)) {
#  mushroom[is.na(mushroom[,var]),var] <- Mode(mushroom[,var], na.rm = TRUE)
#}
#after data preparation of missing values.
#missmap(mushroom, main = "After data preparation of missing values")
ComputeProportion <- function(target,attribute_dataset,Columns,centroid){
len_attr <- length(Columns)
RMSE <- NULL
for(i in 1:len_attr){
tab <- table(target,attribute_dataset[,Columns[i]])
prop <- tab[2,]/(tab[1,]+tab[2,])
centr_prop <- prop - centroid
Err <- sqrt(mean(centr_prop*centr_prop))
RMSE <-rbind(RMSE,Err)
}
norm_err <- (RMSE-min(RMSE))/(max(RMSE)-min(RMSE))
err_mat <- cbind(colnames(mushroom[,Columns]),RMSE)
err_mat <- cbind(err_mat,norm_err)
colnames(err_mat) <- c("Column name","RMSE","Norm. Error")
err_mat
}
proportions_tab <- ComputeProportion(mushroom[,1],mushroom,c(2:23),0.482)
print(proportions_tab)
#stalk_shape ve veil_type en düşük değere sahip
drops <- c("veil_type","stalk_shape") #there is one unique values of veil_type, stalk_shape we can remove this column in our dataset.
mushroom <- mushroom[ , !(names(mushroom) %in% drops)] #remove veil_type,stalk_shape
# remove unncessary data
rm(drops)
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(1579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
#start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(1579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
#start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
#NA değerler var, bunun sebebi birden fazla attribute un iyi bir şekilde eşleşmesi
#bu durumu istemiyoruz o yüzden NA'lı attributeları kaldırıyoruz
drops <- c("stalk_color_above_ring","stalk_color_below_ring","veil_color","ring_number"
,"ring_type","spore_print_color","habitat") #we remove this column in our dataset.
mushroom <- mushroom[ , !(names(mushroom) %in% drops)] #remove
#Warning: glm.fit: algorithm did not converge hatası iterasyon sayısı ile ilgili
#default olarak maxit=25'dir biz 100 yapıyoruz
model <- glm(class ~ ., data = train,family = binomial,maxit = 100)
summary(model)
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(1579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
#start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
#NA değerler var, bunun sebebi birden fazla attribute un iyi bir şekilde eşleşmesi
#bu durumu istemiyoruz o yüzden NA'lı attributeları kaldırıyoruz
drops <- c("stalk_color_above_ring","stalk_color_below_ring","veil_color","ring_number"
,"ring_type","spore_print_color","habitat") #we remove this column in our dataset.
train <- train[ , !(names(train) %in% drops)] #remove
test <- test[ , !(names(test) %in% drops)] #remove
#Warning: glm.fit: algorithm did not converge hatası iterasyon sayısı ile ilgili
#default olarak maxit=25'dir biz 100 yapıyoruz
model <- glm(class ~ ., data = train,family = binomial,maxit = 100)
summary(model)
glm.pred  <- ifelse(predict(model, type = "response") > 0.5,"p","e")
train_err <- mean(glm.pred != train$class)
#install.packages("Amelia")
#install.packages("caret")
library(arm)
library(caret)
library(purrr)
library(Amelia)
#include data preparation file
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(1579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
#start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
#NA değerler var, bunun sebebi birden fazla attribute un iyi bir şekilde eşleşmesi
#bu durumu istemiyoruz o yüzden NA'lı attributeları kaldırıyoruz
drops <- c("stalk_color_above_ring","stalk_color_below_ring","veil_color","ring_number"
,"ring_type","spore_print_color","habitat") #we remove this column in our dataset.
train <- train[ , !(names(train) %in% drops)] #remove
test <- test[ , !(names(test) %in% drops)] #remove
#Warning: glm.fit: algorithm did not converge hatası iterasyon sayısı ile ilgili
#default olarak maxit=25'dir biz 100 yapıyoruz
model <- glm(class ~ ., data = train,family = binomial,maxit = 100)
summary(model)
glm.pred  <- ifelse(predict(model, type = "response") > 0.5,"p","e")
actual <- test[,1]
confusion_matrix <- table(predicted,actual)
train_err <- mean(glm.pred != train$class)
TP <- confusion_matrix[2,2]
TN <- confusion_matrix[1,1]
FP <- confusion_matrix[2,1]
FN <- confusion_matrix[1,2]
Accuracy <- (TP+TN)*100/(TP+TN+FP+FN)
print(Accuracy)
Recall <- TP*100/(TP+FN)
print(Recall)
F_Score <- 2*TP*100/(2*TP+FP+FN)
print(F_Score)
Precision <- TP*100/(TP+FP)
print(Precision)
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(1579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
#start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
#NA değerler var, bunun sebebi birden fazla attribute un iyi bir şekilde eşleşmesi
#bu durumu istemiyoruz o yüzden NA'lı attributeları kaldırıyoruz
drops <- c("stalk_color_above_ring","stalk_color_below_ring","veil_color","ring_number"
,"ring_type","spore_print_color","habitat") #we remove this column in our dataset.
train <- train[ , !(names(train) %in% drops)] #remove
test <- test[ , !(names(test) %in% drops)] #remove
#Warning: glm.fit: algorithm did not converge hatası iterasyon sayısı ile ilgili
#default olarak maxit=25'dir biz 100 yapıyoruz
model <- glm(class ~ ., data = train,family = binomial,maxit = 100)
summary(model)
glm.pred  <- ifelse(predict(model, type = "response") > 0.5,"p","e")
actual <- test[,1]
train_err <- mean(glm.pred != train$class)
confusion_matrix <- table(glm.pred,actual)
TP <- confusion_matrix[2,2]
TN <- confusion_matrix[1,1]
FP <- confusion_matrix[2,1]
FN <- confusion_matrix[1,2]
Accuracy <- (TP+TN)*100/(TP+TN+FP+FN)
print(Accuracy)
Recall <- TP*100/(TP+FN)
print(Recall)
F_Score <- 2*TP*100/(2*TP+FP+FN)
print(F_Score)
Precision <- TP*100/(TP+FP)
print(Precision)
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(1579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
#start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
#NA değerler var, bunun sebebi birden fazla attribute un iyi bir şekilde eşleşmesi
#bu durumu istemiyoruz o yüzden NA'lı attributeları kaldırıyoruz
drops <- c("stalk_color_above_ring","stalk_color_below_ring","veil_color","ring_number"
,"ring_type","spore_print_color","habitat") #we remove this column in our dataset.
train <- train[ , !(names(train) %in% drops)] #remove
test <- test[ , !(names(test) %in% drops)] #remove
#Warning: glm.fit: algorithm did not converge hatası iterasyon sayısı ile ilgili
#default olarak maxit=25'dir biz 100 yapıyoruz
model <- glm(class ~ ., data = train,family = binomial,maxit = 100)
summary(model)
glm.pred  <- ifelse(predict(model, type = "response") > 0.5,"p","e")
glm.pred  <- gsub("TRUE","p",glm.pred )
glm.pred  <- gsub("FALSE","e",glm.pred )
actual <- test[,1]
train_err <- mean(glm.pred != train$class)
confusion_matrix <- table(glm.pred,actual)
TP <- confusion_matrix[2,2]
TN <- confusion_matrix[1,1]
FP <- confusion_matrix[2,1]
FN <- confusion_matrix[1,2]
Accuracy <- (TP+TN)*100/(TP+TN+FP+FN)
print(Accuracy)
Recall <- TP*100/(TP+FN)
print(Recall)
F_Score <- 2*TP*100/(2*TP+FP+FN)
print(F_Score)
Precision <- TP*100/(TP+FP)
print(Precision)
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(1579642)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
#start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
#NA değerler var, bunun sebebi birden fazla attribute un iyi bir şekilde eşleşmesi
#bu durumu istemiyoruz o yüzden NA'lı attributeları kaldırıyoruz
drops <- c("stalk_color_above_ring","stalk_color_below_ring","veil_color","ring_number"
,"ring_type","spore_print_color","habitat") #we remove this column in our dataset.
train <- train[ , !(names(train) %in% drops)] #remove
test <- test[ , !(names(test) %in% drops)] #remove
#Warning: glm.fit: algorithm did not converge hatası iterasyon sayısı ile ilgili
#default olarak maxit=25'dir biz 100 yapıyoruz
model <- glm(class ~ ., data = train,family = binomial,maxit = 100)
summary(model)
#şimdi modeli test datamız üzerinde test ediyoruz
predicted <- predict.glm(model,newdata = test[,-1],type = "response")
#tahmin edilen değerlerin 0.5 üzeri olanlar poison
predicted <- predicted >= 0.5
predicted <- gsub("TRUE","p",predicted)
predicted <- gsub("FALSE","e",predicted)
actual <- test[,1]
train_err <- mean(predicted != train$class)
confusion_matrix <- table(predicted,actual)
TP <- confusion_matrix[2,2]
TN <- confusion_matrix[1,1]
FP <- confusion_matrix[2,1]
FN <- confusion_matrix[1,2]
Accuracy <- (TP+TN)*100/(TP+TN+FP+FN)
print(Accuracy)
Recall <- TP*100/(TP+FN)
print(Recall)
F_Score <- 2*TP*100/(2*TP+FP+FN)
print(F_Score)
Precision <- TP*100/(TP+FP)
print(Precision)
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(45)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
#start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
#NA değerler var, bunun sebebi birden fazla attribute un iyi bir şekilde eşleşmesi
#bu durumu istemiyoruz o yüzden NA'lı attributeları kaldırıyoruz
drops <- c("stalk_color_above_ring","stalk_color_below_ring","veil_color","ring_number"
,"ring_type","spore_print_color","habitat") #we remove this column in our dataset.
train <- train[ , !(names(train) %in% drops)] #remove
test <- test[ , !(names(test) %in% drops)] #remove
#Warning: glm.fit: algorithm did not converge hatası iterasyon sayısı ile ilgili
#default olarak maxit=25'dir biz 100 yapıyoruz
model <- glm(class ~ ., data = train,family = binomial,maxit = 100)
summary(model)
#şimdi modeli test datamız üzerinde test ediyoruz
predicted <- predict.glm(model,newdata = test[,-1],type = "response")
#tahmin edilen değerlerin 0.5 üzeri olanlar poison
predicted <- predicted >= 0.5
predicted <- gsub("TRUE","p",predicted)
predicted <- gsub("FALSE","e",predicted)
actual <- test[,1]
train_err <- mean(predicted != train$class)
confusion_matrix <- table(predicted,actual)
TP <- confusion_matrix[2,2]
TN <- confusion_matrix[1,1]
FP <- confusion_matrix[2,1]
FN <- confusion_matrix[1,2]
Accuracy <- (TP+TN)*100/(TP+TN+FP+FN)
print(Accuracy)
Recall <- TP*100/(TP+FN)
print(Recall)
F_Score <- 2*TP*100/(2*TP+FP+FN)
print(F_Score)
Precision <- TP*100/(TP+FP)
print(Precision)
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(48425)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
#start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
#NA değerler var, bunun sebebi birden fazla attribute un iyi bir şekilde eşleşmesi
#bu durumu istemiyoruz o yüzden NA'lı attributeları kaldırıyoruz
drops <- c("stalk_color_above_ring","stalk_color_below_ring","veil_color","ring_number"
,"ring_type","spore_print_color","habitat") #we remove this column in our dataset.
train <- train[ , !(names(train) %in% drops)] #remove
test <- test[ , !(names(test) %in% drops)] #remove
#Warning: glm.fit: algorithm did not converge hatası iterasyon sayısı ile ilgili
#default olarak maxit=25'dir biz 100 yapıyoruz
model <- glm(class ~ ., data = train,family = binomial,maxit = 100)
summary(model)
#şimdi modeli test datamız üzerinde test ediyoruz
predicted <- predict.glm(model,newdata = test[,-1],type = "response")
#tahmin edilen değerlerin 0.5 üzeri olanlar poison
predicted <- predicted >= 0.5
predicted <- gsub("TRUE","p",predicted)
predicted <- gsub("FALSE","e",predicted)
actual <- test[,1]
train_err <- mean(predicted != train$class)
confusion_matrix <- table(predicted,actual)
TP <- confusion_matrix[2,2]
TN <- confusion_matrix[1,1]
FP <- confusion_matrix[2,1]
FN <- confusion_matrix[1,2]
Accuracy <- (TP+TN)*100/(TP+TN+FP+FN)
print(Accuracy)
Recall <- TP*100/(TP+FN)
print(Recall)
F_Score <- 2*TP*100/(2*TP+FP+FN)
print(F_Score)
Precision <- TP*100/(TP+FP)
print(Precision)
source(file = "Preparation.R") # just shows worked dataset after data preparation.
TRAIN_SIZE  = 0.8
#NUM_OF_FOLD = 10
#TRAIN_DATA  = list()
#show current dataset
#head(mushroom)
#summary(mushroom)
#begin the logistic function (Model fitting)
set.seed(48425)  #Set the seed for reproducibility
train_index <- sample(1:nrow(mushroom), size=nrow(mushroom)*TRAIN_SIZE) # randomly choice rows
test  <- mushroom[-train_index,]
train <- mushroom[train_index,]
start_time <- Sys.time()
model = glm(formula = class ~ ., data = train, family = binomial())
summary(model)
#NA değerler var, bunun sebebi birden fazla attribute un iyi bir şekilde eşleşmesi
#bu durumu istemiyoruz o yüzden NA'lı attributeları kaldırıyoruz
drops <- c("stalk_color_above_ring","stalk_color_below_ring","veil_color","ring_number"
,"ring_type","spore_print_color","habitat") #we remove this column in our dataset.
train <- train[ , !(names(train) %in% drops)] #remove
test <- test[ , !(names(test) %in% drops)] #remove
#Warning: glm.fit: algorithm did not converge hatası iterasyon sayısı ile ilgili
#default olarak maxit=25'dir biz 100 yapıyoruz
model <- glm(class ~ ., data = train,family = binomial,maxit = 100)
summary(model)
#şimdi modeli test datamız üzerinde test ediyoruz
predicted <- predict.glm(model,newdata = test[,-1],type = "response")
#tahmin edilen değerlerin 0.5 üzeri olanlar poison
predicted <- predicted >= 0.5
predicted <- gsub("TRUE","p",predicted)
predicted <- gsub("FALSE","e",predicted)
actual <- test[,1]
train_err <- mean(predicted != train$class)
confusion_matrix <- table(predicted,actual)
TP <- confusion_matrix[2,2]
TN <- confusion_matrix[1,1]
FP <- confusion_matrix[2,1]
FN <- confusion_matrix[1,2]
Accuracy <- (TP+TN)*100/(TP+TN+FP+FN)
print(Accuracy)
Recall <- TP*100/(TP+FN)
print(Recall)
F_Score <- 2*TP*100/(2*TP+FP+FN)
print(F_Score)
Precision <- TP*100/(TP+FP)
print(Precision)
end_time <- Sys.time()
print(paste("running time => ", (end_time-start_time)))
